{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5966aa62-809b-4392-8e81-935597072a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bea24ac-beef-48f5-aafb-f57d0fc40a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.255])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310934da-2641-47ba-acb6-815243930f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,image_paths,transform=None):\n",
    "        self.image_paths=image_paths\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self,idx):\n",
    "        image_path=self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "        label = 1\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8501b40-504c-4cfc-b2dd-a2d916d0a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_path = r\"C:\\Users\\oppor\\Desktop\\RajeshHamal\"\n",
    "celebrity_images = [os.path.join(celeb_path, filename) for filename in os.listdir(celeb_path) if filename.endswith(\".jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01925ee6-ec99-4be9-a123-a2185585f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rajeshdai = Dataset(image_paths=celebrity_images,transform=transform)\n",
    "data_loader=DataLoader(rajeshdai,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b62f81-8c32-40de-8c83-6cedc6d266bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oppor\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oppor\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\oppor/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [04:50<00:00, 161kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Binary output (0 or 1)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b71d35c8-06e1-4291-b591-4dd750668d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d2363e-b874-4ff8-925b-dcd45d4ae5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6639777421951294\n",
      "Epoch 2/5, Loss: 0.69481360912323\n",
      "Epoch 3/5, Loss: 0.28432077169418335\n",
      "Epoch 4/5, Loss: 0.16535209119319916\n",
      "Epoch 5/5, Loss: 0.09148901700973511\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Squeeze to remove the extra dimension\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f30bc68-bc5f-4ce5-99db-f8842e2bf9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not the celebrity.\n"
     ]
    }
   ],
   "source": [
    "# Prediction function\n",
    "def predict(image_path, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Apply the transformation and add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Get model output (probability score)\n",
    "        output = output.item()  # Convert to Python scalar\n",
    "    \n",
    "    # Output: 1 if the model recognizes the celebrity, 0 if not\n",
    "    if output > 0.5:\n",
    "        return \"This is the rajeshhamal!\"\n",
    "    else:\n",
    "        return \"This is not the celebrity.\"\n",
    "\n",
    "# Test prediction with a new image\n",
    "print(predict(r\"C:\\Users\\oppor\\Desktop\\download.jpg\", model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b91f0283-ddad-4296-bf20-4ca46aca4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'rajesh_hamal_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043a7d0-502d-4762-bd09-e9aeef721489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
